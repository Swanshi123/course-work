---
title: "first assignment for 7003"
output:
  html_document: default
  pdf_document: default
date: "2024-09-21"
---

- **student no:303624496**
- **student name: Liu sheng**



## Q1
#### (a)
- ii is correct
- We can find that the coefficient of level is 35, which is a positive number. When the educational level is college (x=1), it shows that college education has a positive effect on salary increase, which is higher than high school level (x=0).

#### (b)
- $$y=50+20X_1+0.07X_2+35X_3+0.01X_4+(-10)X_5$$
- when IQ=110 and GPA=4.0, Y=50+ 20 * 4.0 + 0.07 * 110 + 35 *  1+ 0.01*110*4.0-10*4.0
- 133.8

#### (c)
- False. When we judge whether the model has an interaction effect, we should pay attention to whether the p-value of the interaction coefficient is credible within a certain level.



## Q2
#### (a)&(b)
- Through the following simulation, we can find that when x and y satisfy a linear relationship, adding quadratic and cubic terms will reduce the RSS. However, if we do not know the original data, we cannot predict the change of RSS.
```{r}
# help to reproduce results
set.seed(12)

# Generate random numbers
x <- 1:100  # x must be in order, otherwise the lines will not be connected in order.
y <- 10.7+2*x+rnorm(100,mean=0,sd=20)

# Scatter plot
plot(x, y, main = "Scatter Plot of X and Y", xlab = "X values", ylab = "Y values")
model<-lm(y~x)
model1 <- lm(y ~ x + I(x^2))
model2<-lm(y~x+I(x^2)+I(x^3))
abline(model, col = "red") # 
lines(x,predict(model1),col='blue')#predict fuction
lines(x,predict(model2),col='yellow')
#Calculate different RSS
sum(resid(model)^2)
sum(resid(model1)^2)
sum(resid(model2)^2)
```

#### (c)&(d)
-Through the following simulation, we can find that when x and y do not satisfy the linear relationship, adding quadratic and cubic terms will make the RSS drop more significantly. When it is known that the original variables do not satisfy the linear conditions, we expect that adding square and cubic terms will improve the model.
```{r include=TRUE}
# help to reproduce results
set.seed(12)

# Generate random numbers
x <- 1:100  # x must be in order, otherwise the lines will not be connected in order.
y <- 10.7+x^2+rnorm(100,mean=0,sd=20)

# Scatter plot
plot(x, y, main = "Scatter Plot of X and Y", xlab = "X values", ylab = "Y values")
model<-lm(y~x)
model1 <- lm(y ~ x + I(x^2))
model2<-lm(y~x+I(x^2)+I(x^3))
abline(model, col = "red") # 添加红色回归线
lines(x,predict(model1),col='blue')#predict fuction
lines(x,predict(model2),col='yellow')
# Calculate different RSS
sum(resid(model)^2)
sum(resid(model1)^2)
sum(resid(model2)^2)
```



## Q3
#### (a)
- this is the multiple model by R
```{r}
data <- read.csv("https://raw.githubusercontent.com/kwan-MSDA/MSDA7003/main/Carseats.csv")
###
y<-data$Sales
#table(data$Urban)
x1<-data$Price
x2<-as.factor(data$Urban)
x3<-as.factor(data$US)
modelsales<-lm(y~x1+x2+x3)
summary(modelsales)
```

#### (b)
- x1:show that sales will decrease(-0.054459) when the prices go up.this conculison is statistically reliable at the 99 percent confidence level.

#### (c)
- this is the equation form:
- y=13.04-0.05$x_1$-0.02$x_2$+1.20$x_3$+$\epsilon$ 

$$
f(x) = \begin{cases} 
13.04-0.05*x1-0.02*x_2+1.20*x_3+\epsilon & \text{if } x_2 = 1 & x_3=1 \\
13.04-0.05*x1-0.02*x_2+\epsilon & \text{if } x_2=  1 & x_3=0 \\
13.04+1.20*x_3+\epsilon & \text{if } x_2 = 0 &x_3=1  \\
13.04-0.05*x1+\epsilon & \text{if } x_2 =0  &x_3=0  \\
\end{cases}
$$

#### (d)
- null hypothesis of x1:  $\beta_1=0$
- null hypothesis of X3:  $\beta_3=0$
- We can reject the null hypothesis of x1 and x3 at a significance level of 0.01 (p<0.01)

#### (e)
- According to the previous regression results, removing x2 and the new model is as follows:
```{r}
modelsales1<-lm(y~x1+x3)
summary(modelsales1)
```

#### (f)
- Model (e) is superior to model (a) in terms of the overall explanation of the whole model (r^2) and the significance of the parameters(p-value).

#### (g)
- we can caculate confidence intervals:
```{r}
confint(modelsales1, level = 0.95)
```


## Q4
```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2+2*x1+0.3*x2+rnorm(100)
```

#### (a)
$$y=\beta_0+\beta_1x1+\beta_2x2+\epsilon\\$$
- the regression coefficients is :
$$
\begin{cases}
\beta_0=2\\
\beta_1=2\\
\beta_3=0.3\\
\end{cases}
$$
#### (b)
- correlation:
```{r}
cor(x1, x2, method = "pearson")
```
- plot:
```{r}
plot(x1,x2)
title(main = "x1 vs x2")
```

#### (c)
```{r}
model4c <- lm(y ~ x1+x2)
summary(model4c)
```
- the regression coefficients:
$$
\begin{cases}
\widetilde{\beta}_0=2.1305  \\
\widetilde{\beta}_1=1.4396  \\
\widetilde{\beta}_2=1.0097  \\
\end{cases}
$$
- We found that the coefficients of the regression results are the same as the original construction coefficients\
1.$\widetilde{\beta}_0$ We reject the null hypothesis with 99.9 percent confidence.(p<0.01)\
2.$\widetilde{\beta}_2$ we can not reject the null hypothesis


#### (d)
```{r}
model4d <- lm(y ~ x1)
summary(model4d)
```
- yes, we can(p<0.1)

#### (e)
```{r}
model4e <- lm(y ~ x2)
summary(model4e)
```
- yes,we can(p<0.1)

#### (f)
- No contradiction.
- In the original model, there is a linear relationship between x1 and x2. This will lead to multicollinearity problems between x1 and x2 in model (c). This affects the interpretability of the model.

#### (g)
- The newly added values affect the model in many ways (parameters, significance of parameter estimates, model interpretability)
- In the entire model, this observation is an outlier，We can find out from the scatter plot
```{r}
x1<-c(x1,0.1)
x2<-c(x2,0.8)
y<-c(y,6)

model4g <- lm(y ~ x1+x2)
summary(model4g)

model4g1 <- lm(y ~ x1)
summary(model4g1)

model4g2 <- lm(y ~ x2)
summary(model4g2)
```

```{r}
plot(x1,x2)
```